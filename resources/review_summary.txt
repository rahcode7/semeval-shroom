ChainPoll is a novel hallucination detection methodology that substantially out-performs existing alternatives. It uses two new metrics - Adherence and Correctness - to quantify LLM hallucinations. Adherence measures an LLM's reasoning abilities within provided documents and context, while Correctness captures general logical and reasoning-based mistakes. The study shows that ChainPoll achieves superior performance across all benchmarks in RealHall with an aggregate AUROC of 0.781, beating the next best theoretical algorithm by 11%, and industry standards by over 23%.

Guerreiro et al. present a comprehensive study on hallucinations in neural machine translation (NMT). The authors work with in-domain data, annotate a dataset of over 3.4k sentences for different types of errors and hallucinations, and evaluate various detection methods. They find that previously used methods are largely inadequate and that sequence log-probability performs best. They also propose DEHALLUCINATOR, a method to reduce hallucinations at test time, which significantly reduces the hallucination rate.

Li et al. ntroducethe Hallucination Evaluation benchmark forLarge Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the perfor-mance of LLMs in recognizing hallucination. Their work shows that models like ChatGPT are likely to generate fabricated output. Their work suggests t providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations.
