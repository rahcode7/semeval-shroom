# -*- coding: utf-8 -*-
"""Gradient Tree Model Aware.ipynb

Automatically generated by Colaboratory.

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import json

# Load feature files
feature_files = [f'/content/model-aware-500-{i}.json' for i in range(1, 9)]
feature_dfs = [pd.read_json(file) for file in feature_files]

# Merge feature files into a single DataFrame
merged_features_df = pd.concat(feature_dfs, ignore_index=True)

# Load target file
target_file = 'val.500.model-aware.json'
target_df = pd.read_json(target_file)

# Merge features with target on 'id'
merged_df = pd.merge(merged_features_df, target_df[['id', 'p(Hallucination)']], on='id')

merged_df.head(20)

# Preprocessing

# Add a new column for the sequence
merged_df['sequence'] = merged_df.groupby('id').cumcount() + 1

# Pivot the DataFrame
pivoted_df = merged_df.pivot(index='id', columns='sequence', values='p(Hallucination)_x')

# Rename the columns
pivoted_df.columns = [f'p(Hallucination)_{i}' for i in range(1, len(pivoted_df.columns)+1)]

# Reset the index
pivoted_df.reset_index(inplace=True)

# Merge the pivoted DataFrame with the original DataFrame to get the 'p(Hallucination)_y' column
final_df = pd.merge(pivoted_df, merged_df[['id', 'p(Hallucination)_y']].drop_duplicates(), on='id')

# Display the DataFrame
pd.set_option('display.max_columns', None)

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Separate features and target
X = final_df.drop(['id', 'p(Hallucination)_y'], axis=1)
y = final_df['p(Hallucination)_y']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 150, 200, 250],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5, 7, 10],
    'min_child_weight': [1, 3, 5],
    'gamma': [0, 0.2, 0.4],
    'subsample': [0.6, 1.0],
    'colsample_bytree': [0.6, 1.0],
    'reg_alpha': [0, 0.4],
    'reg_lambda': [1, 1.4],
    'tree_method': ['hist'],  # Updated tree method
    'device': ['cuda']       # Specify the device as cuda
}

# Create a XGBoost regressor object
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror')

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=10, n_jobs=-1, verbose=2)

# Perform the search
grid_search.fit(X_train, y_train)

# Best parameters
print(f"Best parameters: {grid_search.best_params_}")

# Predict and evaluate the best model
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)

# Evaluation metrics
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared (RÂ²): {r2}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Discretizing the predictions using 0.5 as a cutoff
binary_predictions = np.where(predictions > 0.5, 1, 0)
binary_y_test = np.where(y_test > 0.5, 1, 0)

# Binary classification metrics
accuracy = accuracy_score(binary_y_test, binary_predictions)
precision = precision_score(binary_y_test, binary_predictions)
recall = recall_score(binary_y_test, binary_predictions)
f1 = f1_score(binary_y_test, binary_predictions)
conf_matrix = confusion_matrix(binary_y_test, binary_predictions)

# Print binary classification metrics
print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')
print(f'Confusion Matrix:\n{conf_matrix}')

import joblib
# Save the model
model_filename = 'best_gradient_trees_aware.joblib'
joblib.dump(best_model, model_filename)
print(f"Model saved as {model_filename}")

from google.colab import files
files.download('best_gradient_trees_aware.joblib')

import joblib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import json
best_gradient_model = joblib.load('best_gradient_trees_aware.joblib')

#Load feature files
feature_files = [f'/content/model-aware-1500-{i}.json' for i in range(1, 9)]
feature_dfs = [pd.read_json(file) for file in feature_files]

# Merge feature files into a single DataFrame
merged_features_df = pd.concat(feature_dfs, ignore_index=True)

merged_features_df.sort_values('id').head(20)

# Preprocessing

# Group by 'id' and get the list of p(Hallucination) for each group
grouped = merged_features_df.groupby('id')['p(Hallucination)'].apply(list)

# Create new dataframe from the grouped data
new_df = pd.DataFrame(grouped)

# Create new columns p(Hallucination)_1, p(Hallucination)_2, ...
new_df = pd.DataFrame(new_df['p(Hallucination)'].to_list(), index=new_df.index)

# Rename the columns
new_df.columns = [f'p(Hallucination)_{i+1}' for i in range(len(new_df.columns))]

# Reset the index
new_df.reset_index(inplace=True)

# Display the DataFrame
pd.set_option('display.max_columns', None)
final_df=new_df
final_df.head(20)

y_pred = best_gradient_model.predict(final_df.drop('id', axis=1))
y_pred = np.round(y_pred / 0.2) * 0.2
import numpy as np
import pandas as pd

result = []
for i in range(len(final_df)):
    id = str(final_df.loc[i, 'id'])
    p_hallucination = y_pred[i]
    label = 'Hallucination' if p_hallucination > 0.5 else 'Not Hallucination'
    result.append({"id": id, "p(Hallucination)": p_hallucination, "label": label})

# Convert numpy float32 to Python float
def convert(o):
    if isinstance(o, np.float32):
        return float(o)
    raise TypeError

with open('test.model-aware.json', 'w') as f:
  json.dump(result, f, default=convert)

y_pred

